%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.3 (9/9/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage{graphicx}
\usepackage{lipsum} % Package to generate dummy text throughout this template

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them
\usepackage{amsmath}
\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\renewcommand\thesubsubsection{\roman{subsubsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles
\titleformat{\subsubsection}[block]{}{\thesubsubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{PHSX567 Final Project Proposal $\bullet$ \today } % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text


%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{MOSES Data Inversion With Convolutional Neural Networks}} % Article title

\author{
\large
\textsc{Roy Smart}\\[2mm] % Your name
\normalsize Montana State University \\ % Your institution
\normalsize \href{mailto:roytsmart@gmail.com}{roytsmart@gmail.com} % Your email address
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

\thispagestyle{fancy} % All pages have headers and footers



%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2} % Two-column layout throughout the main article text

\section{Introduction}
The \textit{Multi-Order Solar EUV Spectrograph} (MOSES) is a unique instrument developed by Charles Kankelborg's research group at Montana State University that captures solar images in three diffraction orders: $m=0,+1,$ and $-1$. Unlike most spectrographs, MOSES is not equipped with a slit to restrict the field of view on the dispersion axis. The advantage of this configuration is that spectral and spatial information is simultaneously viewed by the instrument, allowing interesting solar features to be more easily identified. However this lack of spatial restriction by a slit means that spectral and spatial information are convolved together to form the images captured by MOSES. These images are known as \textit{overlappograms}. \par
\begin{figure}[H]
	\includegraphics[width=\linewidth]{images/instrument.eps}
	\caption{Layout of the MOSES instrument demonstrating how different wavelengths are overlapped onto each detector \cite{moses}.}
\end{figure}
The main objective of the MOSES instrument is to determine Doppler shifts of the structures observed in the solar transition region. To find this quantity, we must perform what we call an \textit{inversion}. This is best described as taking the 2-dimensional spectral and spatial information from each of the three detectors and constructing a spectral \textit{cube} in three dimensions, with two spatial dimensions and one spectral dimension.
\begin{figure}[H]
	\includegraphics[width=\linewidth]{images/inversion}
	\caption{Diagram demonstrating how compact objects in the data cube are convolved to form the images at each detector \cite{moses}.}
\end{figure}
The main challenge with inverting the MOSES data is that it is an obviously ill-posed problem, i.e. the spectral cube contains more information than is provided by the instrument. This is mathematically described using the expression
\begin{equation}
I_m(x',y') = \int_B v(x'-m \lambda,y',\lambda)d\lambda. \label{invert}
\end{equation}
Where $I(x',y')$ is the intensity measured by the instrument, $v(x',y',\lambda)$ is an object located in the spectral cube, $m$ is the spectral order, $(x',y')$ are the detector coordinates, and the domain $B$ is the passband of the instrument. 
\par Since Equation \eqref{invert} doesn't have a unique solution, there are many possible cubes that could produce the same images captured by MOSES. Consequently, we must use physical constraints to attempt to trim down the large number of potential solutions to the inversion problem. \par 

\section{Motivation}

Many researchers throughout the history of the MOSES research project have developed computational tools to solve the inversion problem. These methods include Smoothed Multiplicative Algebraic Reconstruction Technique (SMART)\cite{smart}, Fourier backprojection and pixon inversion \cite{inversion} and are discussed in detail in the literature. 
\par The above methods have proved successful in revealing the interior structure of the transition region \cite{moses}. However there is still room for progress to be made towards full inversions of the entire MOSES dataset. These algorithms have a tendency to produce what is known as \textit{plaid}. This is a phenomenon where bright objects on the spectral cube tend to be smeared across the direction of the spectral projections, producing decidedly unphysical inversions.

\begin{figure}[H]
	\includegraphics[width=\linewidth]{images/plaid2}
	\caption{An example of plaid. Notice how the bright pixels in the center are smeared diagonally (projection dimension) across the image. This is an example of an unphysical inversion. \cite{tom} }
\end{figure} 

Developing an inversion method that is resistant to plaid would be very beneficial to Kankelborg's research group, as such an inversion would allow researchers to extract more scientific information about the structure of the transition region.

\section{Proposed Inversion Method}
\subsection{Machine Learning}
Thus far, all of the attempts to invert the MOSES data have relied on traditional, human-designed algorithms to carry out the computation. However, in the past few years, great strides have been made in machine learning, where a computer program is trained to solve a particular problem by providing examples of the problem along with each corresponding solution. Feedback is then provided to the network so that it may utilize minimization techniques to converge on a solution. \par I propose that a method based off of machine learning might allow some progress to be made towards the goal of plaid-resistant MOSES inversions. This is made possible by the fact that the learning algorithm could be penalized for producing plaid solutions to the inversion problem and rewarded for producing physical solutions.  \par While machine learning sounds like a fantastic approach to problem solving, there is still a great deal of challenges to contend with. First, the structure of the learning algorithm is very important in determining its problem-solving ability, and unlike traditional algorithms, the right structure can often only be found by trial and error. Furthermore, the learning process takes a large number of examples to converge on the solution. Solving these problems will be discussed in the following pages.

\subsection{Artificial Neural Networks}

Artificial neural networks (ANNs) are machine learning algorithms that are modeled off of the structure of the animal brain. ANNs can be described as a system of \textit{neurons} that have the ability to communicate with each other. The behavior of each neuron is dictated by an activation function. This function maps neuron inputs to outputs, e.g. determines when the neuron is \textit{active}. Active neurons propagate information through the network while dormant neurons restrict it.  \par Communication is facilitated through a series of connections between the neurons, where each connection has an associated weight representing the amount of influence one neuron may have on another. The weights are tuned based off of experience and taken with the activation function, allow the neural network to learn. \par The neurons are usually organized into layers, where each layer can be interpreted as a different computational step used to solve the problem. The first layer is known as the input layer, and the number of input neurons corresponds to the amount of information supplied to the network. The output layer is the last layer, and the number of output neurons equals the amount of information to be expected in the solution of the problem being solved. In between the input and output layers, there are a number of hidden layers used to compute the solution. The amount of hidden layers and the number of neurons per hidden layer depends on the complexity of the problem.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{images/neural}
	\caption{An example of a simple neural network.}
\end{figure}
Using this simple idea, many different species of neural networks may be constructed. The choices of network depth, number of connections, and activation function are examples of what are known as \textit{hyperparameters} which determine the behavior of the network.

\subsection{Convolutional Neural Networks}
\subsubsection{Convolutional Layer}
Vanilla ANNs usually connect each neuron in one layer to every other neuron in the preceding layer. This configuration is sufficient when the number of input neurons is small. However, as the number of input neurons increases (for example, as large as an image), the huge number of free parameters quickly leads to overfitting. Furthermore, the Vanilla ANN treats pixels with large spatial separation equally, which is often unnecessary for image processing where close spatial relationships are more important. \par Convolutional Neural Networks (CNNs) solve this problem by connecting only a small number of the input neurons (known as the receptive field) to the layers below, known as convolutional layers.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{images/Conv_layer}
	\caption{The red box represents the input neurons, with the receptive field connected to the next five convolutional layers.}
\end{figure}
The convolutional layer can be seen as a series of orthogonal filters, with each filter designed to identify a particular feature in an input image. An example of these orthogonal filters can be seen in Figure \ref{ortho}.
\begin{figure}[H]    
  \centering
    \includegraphics[width=0.5\linewidth]{images/ortho}
    \caption{Example of orthogonal filters in convolutional layer. NOTE: This is not from out network.}
    \label{ortho}
\end{figure}
Each filter size corresponds to the size of the receptive fields. The receptive fields overlap, so each filter is applied to every pixel in the input image. Weights are shared between each neuron in the filter so training time can be greatly reduced.

\subsubsection{Max Pooling Layer}
After the convolutional layer, the information is fed into a max pooling layer. This layer has the effect of decreasing the amount of information in the network to control overfitting. 


\section{Implementation}
\subsection{Selecting Hyperparameters}
We are not yet sure how best to select the hyperparameters for the network. Most networks in the literature consist of approximately five convolutional layers, one or more pooling layers, one or more RELU layers, and one fully connected layer. Determining the topography of the network is one of the major challenges that lies ahead.
\subsection{Training Dataset}
The training dataset needs to contain the data gathered by MOSES and the spectral cube that produced the result. Since we do not yet know the answer to the MOSES inversion problem, we will need to look to other sources to provide the training data. An obvious choice is the data collected by the IRIS satellite. IRIS is a traditional slit spectrograph, and thus has independent dimensions for spectral and spatial data. 
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/iris}
	\caption{An example of the data collected by IRIS in the Si IV line. The horizontal dimension is spatial, while the vertical dimension is spectral \cite{iris}.}
\end{figure}
The idea then, is to use the information collected by IRIS to simulate MOSES data. IRIS has collected a large dataset during its mission, so there should be sufficient data available to train the CNN.
\subsection{Hardware}
Training large, intricate neural networks has only recently become tractable for personal computers. While CPUs have only been demonstrating modest floating-point performance gains over the past few years, GPUs have been following an exponential trend. 
\begin{figure}[H]
	\includegraphics[width=\linewidth]{images/ops}
	\caption{Comparison of GFLOP/s for CPU and GPU \cite{nvidia}.}
\end{figure}
Furthermore, GPUs are designed with massive parallelism in mind, which can be utilized by a neural network algorithm. Therefore, it is desirable to take advantage of current GPU performance to quickly train neural networks. 

\subsection{Software}
Developing such an advanced neural network would take a great deal of time, especially for a GPU implementation. Luckily enough, one has already been developed for us; \textit{Caffe} is a convolutional neural network program, developed at Berkley for image processing \cite{caffe}. \par \textit{Caffe} allows the user to define their network as a simple script, and then it trains the network on an Nvidia GPU automatically. Once the network is trained it may be executed on either a CPU or GPU. This will prove useful as we can guarantee that the network will still be usable, regardless of hardware developed in the near future.

\end{multicols}

	\bibliographystyle{unsrt}
	\bibliography{sources}
\end{document}
